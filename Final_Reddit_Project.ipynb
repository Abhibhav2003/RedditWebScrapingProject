{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enter_url(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text,'html')\n",
    "    return soup\n",
    "\n",
    "def Subreddits(soup):\n",
    "    a = soup.find_all('a', class_=\"m-0 font-bold text-12 text-current truncate max-w-[11rem]\")\n",
    "    subreddit_list = []\n",
    "    for i in a:\n",
    "        subreddit_list.append(i.text.strip()) \n",
    "    return subreddit_list\n",
    "\n",
    "def Category(soup):\n",
    "    b = soup.find_all('h6',class_ =\"flex-grow h-md text-12 truncate py-[0.125rem] w-[11rem] m-0\")\n",
    "    Category_list = []\n",
    "    for i in b:\n",
    "        Category_list.append(i.text.strip())\n",
    "    return Category_list\n",
    "\n",
    "def hrefs(soup):\n",
    "    links_hrefs = soup.find_all('a',class_ =\"m-0 font-bold text-12 text-current truncate max-w-[11rem]\")\n",
    "    hrefs_list = [link.get('href') for link in links_hrefs]\n",
    "    return hrefs_list\n",
    "\n",
    "def Subscribers(soup):\n",
    "    f = soup.find_all('faceplate-number')\n",
    "    Subscribers_list = [k.get('number') for k in f]\n",
    "    return Subscribers_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"https://www.reddit.com/best/communities/\"\n",
    "links = []\n",
    "for i in range(1,11,1):\n",
    "        str1 += str(i)\n",
    "        str1 += '/'\n",
    "        links.append(str1)\n",
    "        str1=\"https://www.reddit.com/best/communities/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Subreddits_df = pd.DataFrame()\n",
    "Category_df = pd.DataFrame()\n",
    "hrefs_df = pd.DataFrame()\n",
    "Subscribers_df = pd.DataFrame()\n",
    "\n",
    "def Scraper(links):\n",
    "   global Subreddits_df, Category_df, Subscribers_df, hrefs_df\n",
    "   for url in links:\n",
    "    soup = enter_url(url)\n",
    "    \n",
    "    # Create DataFrames from soup parsing results\n",
    "    subreddit = Subreddits(soup)\n",
    "    subreddit_data = pd.DataFrame(subreddit)\n",
    "    category = Category(soup)\n",
    "    category_data = pd.DataFrame(category)\n",
    "    subscribers = Subscribers(soup)\n",
    "    subscriber_data = pd.DataFrame(subscribers)\n",
    "    hrefss = hrefs(soup)\n",
    "    href_data = pd.DataFrame(hrefss)\n",
    "    \n",
    "    # Append the new DataFrames to the existing DataFrames\n",
    "    Subreddits_df = pd.concat([Subreddits_df, subreddit_data], axis=0)\n",
    "    Category_df = pd.concat([Category_df, category_data], axis=0)\n",
    "    Subscribers_df = pd.concat([Subscribers_df, subscriber_data], axis=0)\n",
    "    hrefs_df = pd.concat([hrefs_df,href_data],axis=0)\n",
    "   \n",
    "   result = pd.concat([Subreddits_df,Category_df,Subscribers_df,hrefs_df],axis=1)\n",
    "   result.columns = ['Subreddits','Category','Subscribers','Links']\n",
    "   result.to_excel('Scraped_Data.xlsx', index=False)\n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scraper(links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
